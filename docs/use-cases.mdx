---
title: "Use Cases"
description: "Real-world applications of OneRun across different industries and team types"
---

OneRun helps teams across industries build better AI agents through systematic evaluation. Here are the most common use cases and how teams leverage the platform.

## Common Use Cases

<Accordion title="E-commerce Support Agents">
**Challenge**: Support agents need to handle order issues, returns, and product questions across diverse customer types.

**OneRun Solution**:
- Generate personas representing different customer segments (first-time buyers, VIP customers, international users)
- Test scenarios like order delays, product defects, and billing disputes
- Evaluate objectives: customer satisfaction, issue resolution rate, response accuracy
</Accordion>

<Accordion title="SaaS Product Support">
**Challenge**: Technical support agents must help users with complex product issues while maintaining high customer satisfaction.

**OneRun Solution**:
- Create personas with varying technical expertise levels
- Test troubleshooting scenarios and feature explanation requests
- Measure technical accuracy, user empowerment, and satisfaction

**Key Metrics**: Resolution rate, escalation frequency, user confidence improvement
</Accordion>

<Accordion title="B2B Sales Qualification">
**Challenge**: Sales agents need to qualify leads effectively while providing value to prospects at different stages.

**OneRun Solution**:
- Generate personas representing different business sizes, industries, and buying stages
- Test discovery conversations and objection handling scenarios
- Evaluate lead qualification accuracy, conversion potential, and relationship building
</Accordion>

<Accordion title="Real Estate Chatbots">
**Challenge**: Property agents must handle inquiries about listings, schedule viewings, and qualify potential buyers/renters.

**OneRun Solution**:
- Create personas with different budgets, preferences, and urgency levels
- Test property recommendation and scheduling scenarios
- Measure lead capture rate, qualification accuracy, and user satisfaction
</Accordion>

<Accordion title="Patient Intake & Triage">
**Challenge**: Healthcare agents need to collect patient information accurately while maintaining empathy and compliance.

**OneRun Solution**:
- Generate personas with different health conditions, ages, and communication preferences
- Test symptom collection and appointment scheduling scenarios
- Evaluate information accuracy, patient comfort, and regulatory compliance

**Key Considerations**: HIPAA compliance, medical accuracy, patient trust
</Accordion>

<Accordion title="Learning Support Assistants">
**Challenge**: Educational agents must provide personalized help while adapting to different learning styles and knowledge levels.

**OneRun Solution**:
- Create student personas across different grade levels and subjects
- Test homework help and concept explanation scenarios
- Measure learning effectiveness, engagement, and comprehension improvement
</Accordion>

<Accordion title="AI Product Teams">

**Primary Goals**: Validate new features, compare model versions, ensure quality at scale

**How They Use OneRun**:
- **A/B Testing**: Compare different agent versions against the same evaluation criteria
- **Feature Validation**: Test new capabilities before production release
- **Quality Assurance**: Maintain consistent performance across agent updates
- **Performance Monitoring**: Track agent quality trends over time

**Success Story**: "We reduced our agent quality incidents by 80% by implementing systematic OneRun evaluations before each deployment."
</Accordion>

<Accordion title="Customer Experience Teams">

**Primary Goals**: Improve customer satisfaction, reduce escalations, maintain brand voice

**How They Use OneRun**:
- **Brand Consistency**: Ensure agents maintain appropriate tone and messaging
- **Scenario Coverage**: Test edge cases that customer service teams encounter
- **Escalation Reduction**: Identify conversation patterns that lead to human handoffs
- **Customer Journey Optimization**: Evaluate agent performance at different touchpoints

**Key Metrics**: Customer satisfaction scores, first-contact resolution, escalation rates
</Accordion>

<Accordion title="AI Researchers">

**Primary Goals**: Benchmark performance, validate approaches, publish reliable results

**How They Use OneRun**:
- **Systematic Evaluation**: Create reproducible testing environments
- **Comparative Analysis**: Benchmark different models and approaches
- **Dataset Generation**: Create high-quality conversation datasets for research
- **Ablation Studies**: Test the impact of different components or techniques

**Research Applications**: Paper validation, grant proposals, peer review preparation
</Accordion>

<Accordion title="Enterprise Development Teams">

**Primary Goals**: Meet compliance requirements, ensure security, maintain operational standards

**How They Use OneRun**:
- **Compliance Testing**: Verify agents meet regulatory requirements
- **Security Validation**: Test agents against adversarial scenarios
- **Documentation**: Generate audit trails for quality processes
- **Risk Management**: Identify potential failure modes before production

**Compliance Areas**: Financial services, healthcare, government contracting
</Accordion>

## Common Implementation Approaches

### Quality Gates
Set performance thresholds that agents must meet before deployment. Teams typically require 80%+ satisfaction scores before releasing updates.

### Progressive Testing
Start with small simulations (10-20 conversations) to validate changes, then scale up to comprehensive evaluations (100+ conversations) before full deployment.

### Multi-Environment Strategy
- **Development**: Quick validation with focused scenarios
- **Staging**: Comprehensive testing at realistic scale  
- **Production**: Ongoing monitoring with key scenarios

## Getting Started with Your Use Case

<Steps>
  <Step title="Identify Your Scenario">
    Define the specific situations your agent handles most frequently
  </Step>
  <Step title="Define Success Metrics">
    Determine what outcomes matter most for your business and users
  </Step>
  <Step title="Start Small">
    Begin with 10-20 conversations to validate your approach
  </Step>
  <Step title="Iterate and Scale">
    Refine your evaluation criteria and gradually increase simulation size
  </Step>
</Steps>

<Note>
The most successful teams start with their most common or highest-risk scenarios, then expand their evaluation coverage over time.
</Note>

<Tip>
Consider creating different evaluation profiles for different aspects of your agent - one focused on accuracy, another on user experience, and a third on edge case handling.
</Tip>